{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4 – Case Study: Analyse & catégorisation des reviews (Version Amazon full dataset)\n",
        "\n",
        "Ce notebook utilise l’ensemble des tables du modèle Amazon (25 tables) pour construire un jeu de données riche de reviews,\n",
        "puis appliquer les méthodes d’analyse demandées au **Step 4** :\n",
        "\n",
        "- Prétraitement NLP des reviews\n",
        "- Clustering non supervisé pour regrouper les avis\n",
        "- Classification *zero-shot* pour catégoriser automatiquement les reviews\n",
        "- (Optionnel) classification supervisée si un label est ajouté\n",
        "- Visualisations et métriques pour le rapport de 5–10 pages\n",
        "\n",
        "> ℹ️ Les tables utilisées en priorité pour les reviews :\n",
        "> - `review.csv` : texte des avis + rating\n",
        "> - `product_reviews.csv` : lien produit ↔ review\n",
        "> - `product.csv` : informations produit\n",
        "> - `seller_reviews.csv` : lien vendeur ↔ review\n",
        "> - `seller.csv` : informations vendeur\n",
        ">\n",
        "> Les autres tables (orders, buyer, etc.) peuvent servir à enrichir l’analyse (segmentation par client, période, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 1. Imports des librairies\n",
        "# ==========================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# modèles avancés\n",
        "# pip install sentence-transformers transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8, 5)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "BASE_PATH = '/mnt/data'  # à adapter si besoin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chargement de toutes les tables du modèle\n",
        "\n",
        "On charge ici les principales tables fournies (25 tables). Adaptable selon ton environnement\n",
        "(Data Lake, Snowflake, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 2. Load des tables CSV\n",
        "# ==========================\n",
        "def load_csv(name: str) -> pd.DataFrame:\n",
        "    path = os.path.join(BASE_PATH, name)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Fichier introuvable: {path}\")\n",
        "    df = pd.read_csv(path)\n",
        "    # suppression éventuelle d'une colonne d'index 'Unnamed: 0'\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.drop(columns=['Unnamed: 0'])\n",
        "    return df\n",
        "\n",
        "buyer            = load_csv('buyer.csv')\n",
        "carrier          = load_csv('carrier.csv')\n",
        "cart             = load_csv('cart.csv')\n",
        "cart_items       = load_csv('cart_items.csv')\n",
        "category         = load_csv('category.csv')\n",
        "customer         = load_csv('customer.csv')\n",
        "customer_payment = load_csv('customer_payment.csv')\n",
        "customer_shipping= load_csv('customer_shipping.csv')\n",
        "daily_deals      = load_csv('daily_deals.csv')\n",
        "discount         = load_csv('discount.csv')\n",
        "orders           = load_csv('orders.csv')\n",
        "payment_details  = load_csv('payment_details.csv')\n",
        "product          = load_csv('product.csv')\n",
        "product_images   = load_csv('product_images.csv')\n",
        "product_reviews  = load_csv('product_reviews.csv')\n",
        "returns          = load_csv('returns.csv')\n",
        "review           = load_csv('review.csv')\n",
        "review_images    = load_csv('review_images.csv')\n",
        "seller           = load_csv('seller.csv')\n",
        "seller_products  = load_csv('seller_products.csv')\n",
        "seller_reviews   = load_csv('seller_reviews.csv')\n",
        "shipment         = load_csv('shipment.csv')\n",
        "shipping_details = load_csv('shipping_details.csv')\n",
        "subscription     = load_csv('subscription.csv')\n",
        "wishlist_item    = load_csv('wishlist_item.csv')\n",
        "\n",
        "len(buyer), len(review), len(product_reviews), len(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Construction d'une vue analytique `reviews_enriched`\n",
        "\n",
        "On crée une table analytique qui enrichit chaque review avec :\n",
        "- les informations produit (`product`)\n",
        "- la catégorie (`category`)\n",
        "- le vendeur (`seller`)\n",
        "- le buyer (client, via `buyer`)\n",
        "\n",
        "Cette vue servira ensuite pour le NLP et les visualisations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 3. Construction de reviews_enriched\n",
        "# ==========================\n",
        "# 1) review + product\n",
        "reviews_prod = review.merge(product_reviews, on='review_id', how='left')\n",
        "reviews_prod = reviews_prod.merge(product, on='p_id', how='left')\n",
        "\n",
        "# 2) ajout catégorie\n",
        "reviews_prod = reviews_prod.merge(category, on='category_id', how='left', suffixes=('', '_cat'))\n",
        "\n",
        "# 3) ajout seller\n",
        "reviews_prod = reviews_prod.merge(seller_reviews, on='review_id', how='left')\n",
        "reviews_prod = reviews_prod.merge(seller, on='seller_id', how='left', suffixes=('', '_seller'))\n",
        "\n",
        "# 4) ajout buyer / orders (optionnel, via buyer_id)\n",
        "reviews_prod = reviews_prod.merge(buyer, on='buyer_id', how='left', suffixes=('', '_buyer'))\n",
        "\n",
        "reviews_enriched = reviews_prod.copy()\n",
        "\n",
        "print(reviews_enriched.shape)\n",
        "reviews_enriched.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Préparation du texte pour le NLP\n",
        "\n",
        "On crée une variable texte unique à partir de :\n",
        "- `title`\n",
        "- `r_desc` (description de la review)\n",
        "- éventuellement `p_name` (nom produit) pour plus de contexte.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 4. Préprocessing texte\n",
        "# ==========================\n",
        "text_cols = []\n",
        "for col in ['title', 'r_desc', 'p_name']:\n",
        "    if col in reviews_enriched.columns:\n",
        "        text_cols.append(col)\n",
        "\n",
        "def build_text(row):\n",
        "    parts = []\n",
        "    for c in text_cols:\n",
        "        val = row.get(c, '')\n",
        "        if isinstance(val, str):\n",
        "            parts.append(val)\n",
        "    return ' '.join(parts)\n",
        "\n",
        "reviews_enriched['raw_text'] = reviews_enriched.apply(build_text, axis=1)\n",
        "reviews_enriched['raw_text'] = reviews_enriched['raw_text'].fillna('')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "lemm = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    tokens = []\n",
        "    for w in text.split():\n",
        "        if w in stop:\n",
        "            continue\n",
        "        tokens.append(lemm.lemmatize(w))\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "reviews_enriched['clean_text'] = reviews_enriched['raw_text'].apply(preprocess)\n",
        "reviews_enriched[['raw_text', 'clean_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. EDA rapide sur les reviews\n",
        "\n",
        "- Distribution de la longueur des reviews\n",
        "- Distribution des ratings\n",
        "- Aperçu des reviews les plus longues\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 5. EDA – longueur, rating\n",
        "# ==========================\n",
        "df = reviews_enriched.copy()\n",
        "df['length'] = df['clean_text'].str.split().apply(len)\n",
        "\n",
        "df['length'].hist(bins=50)\n",
        "plt.title('Distribution de la longueur des reviews')\n",
        "plt.xlabel('Nombre de mots')\n",
        "plt.ylabel('Nombre de reviews')\n",
        "plt.show()\n",
        "\n",
        "if 'rating' in df.columns:\n",
        "    df['rating'].value_counts().sort_index().plot(kind='bar')\n",
        "    plt.title('Distribution des ratings')\n",
        "    plt.xlabel('Rating')\n",
        "    plt.ylabel('Nombre de reviews')\n",
        "    plt.show()\n",
        "\n",
        "df[['raw_text', 'length']].sort_values('length', ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TF-IDF + clustering KMeans\n",
        "\n",
        "On applique TF-IDF puis KMeans pour obtenir des groupes de reviews.\n",
        "On calcule le **silhouette score** pour évaluer la qualité des clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 6. Clustering KMeans\n",
        "# ==========================\n",
        "max_features = 5000\n",
        "vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "X_tfidf = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "k = 6  # nombre de clusters (à ajuster)\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(X_tfidf)\n",
        "\n",
        "sil_score = silhouette_score(X_tfidf, df['cluster'])\n",
        "print(f\"Silhouette score (K={k}): {sil_score:.3f}\")\n",
        "\n",
        "df['cluster'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Répartition des clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Nombre de reviews')\n",
        "plt.show()\n",
        "\n",
        "df.groupby('cluster')['clean_text'].apply(lambda x: ' | '.join(x.iloc[:3])).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Zero-shot classification\n",
        "\n",
        "On utilise un modèle pré-entraîné (`facebook/bart-large-mnli`) pour attribuer\n",
        "un label à chaque review parmi une liste de catégories métier.\n",
        "\n",
        "Exemple de labels (à adapter à ton cas) :\n",
        "- `delivery issue`\n",
        "- `product quality`\n",
        "- `price problem`\n",
        "- `customer service`\n",
        "- `excellent overall`\n",
        "- `useless review`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 7. Zero-shot classification\n",
        "# ==========================\n",
        "zero_shot_classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
        "\n",
        "candidate_labels = [\n",
        "    'delivery issue',\n",
        "    'product quality',\n",
        "    'price problem',\n",
        "    'customer service',\n",
        "    'excellent overall',\n",
        "    'useless review',\n",
        "]\n",
        "\n",
        "def classify_zero_shot(text: str) -> str:\n",
        "    if not isinstance(text, str) or text.strip() == '':\n",
        "        return 'unknown'\n",
        "    result = zero_shot_classifier(text, candidate_labels=candidate_labels)\n",
        "    return result['labels'][0]\n",
        "\n",
        "# pour contrôler la durée de calcul : échantillon\n",
        "sample_size = min(1000, len(df))\n",
        "df_sample = df.sample(sample_size, random_state=42).copy()\n",
        "\n",
        "df_sample['zero_shot_label'] = df_sample['raw_text'].apply(classify_zero_shot)\n",
        "\n",
        "df_sample['zero_shot_label'].value_counts().plot(kind='bar')\n",
        "plt.title('Répartition des labels zero-shot (échantillon)')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Nombre de reviews')\n",
        "plt.show()\n",
        "\n",
        "df_sample[['raw_text', 'zero_shot_label']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Classification supervisée (optionnel)\n",
        "\n",
        "Si tu crées une colonne `label` (par exemple 1 = review utile, 0 = inutile),\n",
        "tu peux entraîner un modèle simple Logistic Regression pour évaluer les performances\n",
        "d’un classifieur supervisé.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 8. Classification supervisée (si df contient 'label')\n",
        "# ==========================\n",
        "if 'label' in df.columns:\n",
        "    print(\"Colonne 'label' trouvée, entraînement d'un modèle supervisé...\")\n",
        "    X = X_tfidf\n",
        "    y = df['label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print('\\nClassification report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print('\\nConfusion matrix:\\n', cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "    ax.set_title('Matrice de confusion')\n",
        "    plt.colorbar(im)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Aucune colonne 'label' trouvée. Ajoute un label manuellement si tu veux tester la partie supervisée.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Embeddings (Sentence Transformers) + PCA (optionnel avancé)\n",
        "\n",
        "On encode les reviews avec un modèle type `all-MiniLM-L6-v2`, puis on visualise\n",
        "les embeddings projetés en 2D à l’aide de PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 9. Embeddings + PCA\n",
        "# ==========================\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "st_model = SentenceTransformer(model_name)\n",
        "\n",
        "sample_size = min(1500, len(df))\n",
        "df_emb = df.sample(sample_size, random_state=42).copy()\n",
        "\n",
        "embeddings = st_model.encode(df_emb['clean_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords = pca.fit_transform(embeddings)\n",
        "\n",
        "df_emb['x'] = coords[:, 0]\n",
        "df_emb['y'] = coords[:, 1]\n",
        "\n",
        "plt.scatter(df_emb['x'], df_emb['y'], c=df_emb.get('cluster', 0), alpha=0.6)\n",
        "plt.title('Projection PCA des embeddings de reviews')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Synthèse & export pour le rapport\n",
        "\n",
        "À partir de ce notebook, tu peux :\n",
        "- extraire des KPIs :\n",
        "  - nombre de reviews par rating, par catégorie produit, par vendeur\n",
        "  - répartition des clusters, des labels zero-shot\n",
        "- exporter des jeux de données agrégés vers Snowflake / ton Data Warehouse\n",
        "- faire des captures d’écran des graphes pour le rapport et la présentation.\n",
        "\n",
        "Tu peux par exemple créer une vue agrégée par produit + catégorie zero-shot :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exemple d'agrégation (à adapter)\n",
        "if 'zero_shot_label' in df_sample.columns:\n",
        "    agg = (\n",
        "        df_sample\n",
        "        .groupby(['p_id', 'zero_shot_label'])\n",
        "        .size()\n",
        "        .reset_index(name='nb_reviews')\n",
        "    )\n",
        "    agg.head()\n",
        "else:\n",
        "    print(\"L'agrégation par zero_shot_label nécessite d'avoir exécuté la cellule de zero-shot.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}