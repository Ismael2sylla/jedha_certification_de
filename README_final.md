# Amazon Review Analysis - Pipeline ETL

Pipeline ETL automatisÃ© pour extraire, transformer et charger les
donnÃ©es d'avis Amazon depuis PostgreSQL vers S3, Snowflake et MongoDB.

ðŸš€ Nouveau sur le projet ? Consultez QUICKSTART.md pour dÃ©marrer en 5
minutes !

------------------------------------------------------------------------

## DÃ©marrage Rapide (3 Ã©tapes)

### 1. DÃ©marrer les bases de donnÃ©es

``` bash
# PostgreSQL (contient les donnÃ©es source)
docker-compose -f docker-compose.postgres.yml up -d

# MongoDB (stocke les mÃ©tadonnÃ©es du pipeline)
cd src_code
docker-compose -f docker-compose.mongodb.yml up -d
cd ..
```

> Attendre 1-2 minutes pour l'initialisation PostgreSQL lors du premier
> lancement.

------------------------------------------------------------------------

### 2. Configurer les credentials

``` bash
cd src_code
cp .env.example .env
# Modifier .env avec vos credentials AWS et Snowflake
```

------------------------------------------------------------------------

### 3. Lancer le pipeline

``` bash
cd src_code

# Option A : Pipeline complet
python scripts/pipeline.py --all

# Option B : Ã‰tape par Ã©tape
python scripts/extract_to_s3.py
python scripts/process_and_store.py
```

------------------------------------------------------------------------

## Architecture

    PostgreSQL (Docker) â†’ AWS S3 (Data Lake) â†’ Snowflake (Data Warehouse)
                                  â†“
                           MongoDB (Docker)

------------------------------------------------------------------------

## Structure du Projet

    project_2/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ docker-compose.postgres.yml
    â”œâ”€â”€ .env.local
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ clean/
    â”œâ”€â”€ docker/postgres/init/
    â””â”€â”€ src_code/
        â”œâ”€â”€ README.md
        â”œâ”€â”€ docker-compose.mongodb.yml
        â”œâ”€â”€ .env
        â”œâ”€â”€ scripts/
        â”‚   â”œâ”€â”€ pipeline.py
        â”‚   â”œâ”€â”€ extract_to_s3.py
        â”‚   â””â”€â”€ process_and_store.py
        â””â”€â”€ config/

------------------------------------------------------------------------

## Commandes Utiles

### Gestion des conteneurs

``` bash
docker ps
docker-compose -f docker-compose.postgres.yml down
cd src_code && docker-compose -f docker-compose.mongodb.yml down
```

### RÃ©initialiser PostgreSQL

``` bash
docker-compose -f docker-compose.postgres.yml down -v
docker-compose -f docker-compose.postgres.yml up -d
```

### VÃ©rifier les connexions

``` bash
# PostgreSQL
docker exec -it amazon_postgres_db psql -U admin -d amazon_db -c "SELECT COUNT(*) FROM product;"

# MongoDB
docker exec -it amazon-reviews-mongodb mongosh -u admin -p changeme --eval "db.adminCommand('ping')"
```

------------------------------------------------------------------------

## DonnÃ©es Disponibles

Le projet contient environ **1,7M d'enregistrements** rÃ©partis dans 25
tables :

-   130â€¯766 clients\
-   42â€¯858 produits\
-   222â€¯644 commandes\
-   111â€¯322 avis\
-   100â€¯000 acheteurs

------------------------------------------------------------------------

## Tests de QualitÃ©

``` bash
cd src_code
python tests/test_data_quality.py
python scripts/generate_quality_report.py
```

8 tests automatisÃ©s :

-   Connexion PostgreSQL\
-   Ratings valides (1--5)\
-   DÃ©tection doublons\
-   Champs obligatoires\
-   Prix positifs\
-   Textes non vides\
-   Types cohÃ©rents\
-   IntÃ©gritÃ© rÃ©fÃ©rentielle

Rapports disponibles : `src_code/reports/`

------------------------------------------------------------------------

## Documentation DÃ©taillÃ©e

-   `src_code/README.md`\
-   `CONFORMITE_ETL.md`\
-   `.env.example`

------------------------------------------------------------------------

## Technologies

  Domaine          Outil
  ---------------- --------------
  Source           PostgreSQL
  Data Lake        AWS S3
  Data Warehouse   Snowflake
  Logging          MongoDB
  ETL              Python 3.11+

------------------------------------------------------------------------

## Ã€ propos du projet

Projet rÃ©alisÃ© dans le cadre du **Bootcamp Data Engineering Jedha**,
dans un contexte d'industrialisation d'un pipeline d'analyse d'avis
Amazon, en cohÃ©rence avec les standards modernes du Data Engineering et
mon parcours de reconversion vers un rÃ´le de **Data Engineer**.
